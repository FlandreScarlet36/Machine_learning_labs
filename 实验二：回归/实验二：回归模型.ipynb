{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验二：回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回归是监督学习的一个重要问题，回归用于预测**输入变量**和**输出变量**之间的关系，特别是当输入变量的值发生变化时，输出变量的值也随之发生变化。\n",
    "- 回归模型是一种表示从输入变量到输出变量之间映射的函数\n",
    "- 对连续值的预测\n",
    "- 可以用合适的曲线揭示样本点随着自变量的变化关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "### 截止日期：10月22日\n",
    "作业的提交格式参考之前的说明，提交到18329300691@163.com\n",
    "### 基本要求\n",
    "将数据集winequality-white.csv按照4:1划分为训练集和测试集。\n",
    "1. 构造线性回归模型，并采用批量梯度下降**和**随机梯度下降进行优化；输出训练集和测试集的均方误差（MSE），画出MSE收敛曲线。\n",
    "1. 对于批量梯度下降**和**随机梯度下降，采用不同的学习率并进行MSE曲线展示，分析选择最佳的学习率。\n",
    "\n",
    "特别需要注意：\n",
    "- 划分数据集时尽可能保持数据分布的一致性，保持样本类别比例相似，可采用分层采样的方式。\n",
    "- 需要对数据集进行一定的预处理\n",
    "\n",
    "### 中级要求\n",
    "探究回归模型在机器学习和统计学上的差异。\n",
    "- 回归模型在机器学习领域和统计学领域中都十分常用，而且使用方法也相似，但其实际的含义具有本质的区别。我们希望同学们从回归模型的角度更加充分地理解机器学习和统计学的区别。\n",
    "\n",
    "\n",
    "### 高级要求\n",
    "编程实现岭回归算法，求解训练样本的岭回归模型，平均训练误差和平均测试误差（解析法、批量梯度下降法和随机梯度下降法**均可**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型(Linear Regression)，因为结构简单，可解释性好，实现简单，在工程领域得到广泛应用。\n",
    "\n",
    "首先对线性函数进行简单的回顾："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a.png](https://s2.loli.net/2022/10/04/rdm5WLI84BNznUR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们明确几个常用的数学符号：\n",
    "- 特征 (features): $x_i$, 比如房屋的面积，卧室的数量都可以是房屋的特征\n",
    "- 特征向量 (输入): $x$, 若干个特征组成的向量，代表一套房屋的所有信息。例如，$x^{(i)}_j$ 表示第 $i$ 套房的第 $j$ 个特征\n",
    "- 输出向量 $y$, $y^{(i)}$ 表示第 $i$ 个输入对应的输出\n",
    "- 假设 (hypothesis): 也称预测函数，比如一个线性的预测函数是：\n",
    "$$h_\\theta (x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n=\\theta^T x$$\n",
    "上述的表达式就是**回归方程 (regression equation)**, $\\theta$ 就是回归系数，关系到我们预测的准确程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一元线性回归 vs. 多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设一共有N个特征向量，对于多元线性回归有m个特征：\n",
    "- 数据集：\n",
    "   - 一元：$\\{(x^{(i)}, y^{(i)})\\}$  \n",
    "   - 多元：$\\{(\\pmb{x}^{(i)}, y^{(i)})\\},\\\\ \\pmb{x}^{(i)}=[x^{(i)}_1, x^{(i)}_2,..., x^{(i)}_m], i=1,2,...,N$\n",
    "\n",
    "\n",
    "- 假设：\n",
    "  - 一元：$f(x^{(i)},\\pmb{\\theta})=\\theta_0+\\theta_1x^{(i)}$ \n",
    "  - 多元：$f(\\pmb{x}^{(i)},\\pmb{\\theta})=\\theta_0+\\theta_1x^{(i)}_1+\\theta_2x^{(i)}_2 + ... +\\theta_mx^{(i)}_m $\n",
    "  \n",
    "- 参数：\n",
    "  - 一元：$\\pmb{\\theta}=[\\theta_0, \\theta_1]$\n",
    "  - 多元：$\\pmb{\\theta}=[\\theta_0, \\theta_1,\\theta_2, ...,\\theta_m]$\n",
    "\n",
    "- 损失函数：\n",
    "  - MSE：$$Loss=\\frac{1}{N}\\sum_{i=1}^N(y^{(i)}-f(\\pmb{x}^{(i)},\\pmb{\\theta}))^2$$\n",
    "  有的资料上损失函数多了个$\\frac{1}{2}$：\n",
    "  $$Loss=\\frac{1}{2N}\\sum_{i=1}^N(y^{(i)}-f(\\pmb{x}^{(i)},\\pmb{\\theta}))^2$$\n",
    "  目的是求导后将二次项的系数变为1，加和不加对结果理论上没有影响。\n",
    "- 目标：损失函数最小\n",
    "\n",
    "- 解析解：\n",
    "  - 一元：分别对MSE中的$\\theta_0, \\theta_1$求偏导\n",
    "  \n",
    "  - 多元：对MSE中的$\\pmb{\\theta}$求偏导\n",
    "  $\\pmb{\\theta}=(\\pmb{x}^T\\pmb{x})^{-1}\\pmb{x}^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化方法\n",
    " - 直接求解析解：$\\pmb{\\theta}=(\\pmb{x}^T\\pmb{x})^{-1}\\pmb{x}^Ty$\n",
    "   - 优点：不需要试错，可以直接取得最小值，比较快捷。\n",
    "\n",
    "   - 缺点：当特征过于复杂时，无法求逆。\n",
    "\n",
    "   - 适用于：小数据场景。（梯度下降相对用的更多一些）\n",
    " - 梯度下降法\n",
    " - 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2022/10/04/XOjpPcdCksRwQn8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2022/10/04/OfuUjWLSJF3qyso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度下降\n",
    "批量梯度下降法为最小化所有训练样本的损失函数，使得最终求解的是全局的最优解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2022/10/04/TB4DmndiJ9OVPYk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然批量梯度下降能够收敛到最小值，但每调节一个$\\theta_j$都必须遍历一遍样本集，如果样本的体积m很大，那么这种算法开销巨大，但由于其向量表示，可以利用并行计算优化性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2022/10/04/u6YUkgCxZ7s48ft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式$\\pmb{\\theta}=(\\pmb{x}^T\\pmb{x})^{-1}\\pmb{x}^Ty$不可逆原因:\n",
    "\n",
    "- 矩阵可逆的充要条件：满秩；\n",
    "- 存在噪声维，使得特征间存在线性关系，导致矩阵的秩小于特征维度；\n",
    "- 特征数比样本还多的时候，方程的个数比未知数的个数还要少，所以会导致矩阵的秩小于样本数,无穷多解满足该情况，进而矩阵不可逆。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决这个问题，岭回归在最小二乘估计的基础上增加了一项，即岭回归估计：\n",
    "$$\\pmb{\\theta}=(\\pmb{x}^T\\pmb{x}+\\lambda\\pmb{I})^{-1}\\pmb{x}^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决这个问题，岭回归在最小二乘估计的基础上增加了一项，即岭回归估计：\n",
    "$$\\pmb{\\theta}=(\\pmb{x}^T\\pmb{x}+\\lambda\\pmb{I})^{-1}\\pmb{x}^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而岭回归模型的目标函数在线性模型的基础上加了L2范数的惩罚项：\n",
    "$$Loss=\\frac{1}{2N}\\sum_{i=1}^N(y^{(i)}-f(\\pmb{x}^{(i)},\\pmb{\\theta}))^2+\\lambda \\sum_{j=0}^N\\theta_j^2$$\n",
    "当岭参数$\\lambda$时，得到最小二乘解，当岭参数$\\lambda$趋向更大时，岭回归系数 $\\pmb{\\theta}$ 估计趋向于0。\n",
    "从岭回归的原理可以知道，岭回归就是改良后的最小二乘估计法，通过放弃最小二乘法的无偏性，通过损失部分特征信息，降低模型精度来得到更符合实际情况的回归系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝜽=〖(𝐗〗^𝑇 〖𝐗+𝜆𝑰)〗^(−𝟏) 𝐗^𝑇 𝐲 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### winequality-white数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
       "0               7.0              0.27         0.36            20.7      0.045  \\\n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
       "0                    45.0                 170.0  1.00100  3.00       0.45  \\\n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"winequality-white.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一列是质量的评级，前面其他的都是酒的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据集进行预处理\n",
    "\n",
    "对数据集的预处理是一个十分重要的步骤，能够使不同量纲的特征处于同一数值量级，减少方差大的特征的影响，使模型更准确，并加快学习算法的收敛速度。\n",
    "\n",
    "常用的预处理方法有：标准化、归一化和中心化。同学们根据需要从中选择一种或几种方法进行预处理（采用其他方法亦可）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2022/10/04/WqxB76cXLGtePSK.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心化代码\n",
    "def Normalization_fun(x):\n",
    "    # 特征零均值\n",
    "    x = (x - np.mean(x, 0)) / (np.max(x, 0) - np.min(x, 0))\n",
    "    return x\n",
    "\n",
    "# 提取特征和标签\n",
    "X = data.iloc[:, 0:-1]  # N D\n",
    "X = Normalization_fun(X)\n",
    "Y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化中心化后的sulphates特征\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(X[\"sulphates\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.219457</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>0.033770</td>\n",
       "      <td>0.073409</td>\n",
       "      <td>0.134425</td>\n",
       "      <td>-0.171151</td>\n",
       "      <td>-0.046334</td>\n",
       "      <td>-0.276495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.053345</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>-0.073488</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>-0.074244</td>\n",
       "      <td>-0.014758</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.101576</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>-0.163591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.119732</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.039644</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>-0.095964</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.065212</td>\n",
       "      <td>-0.057961</td>\n",
       "      <td>-0.066817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>-0.047295</td>\n",
       "      <td>-0.008549</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.110532</td>\n",
       "      <td>0.030319</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>-0.104473</td>\n",
       "      <td>-0.099075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>-0.047295</td>\n",
       "      <td>-0.008549</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.110532</td>\n",
       "      <td>0.030319</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>-0.104473</td>\n",
       "      <td>-0.099075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.062960</td>\n",
       "      <td>-0.066903</td>\n",
       "      <td>-0.026621</td>\n",
       "      <td>-0.073488</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>-0.039401</td>\n",
       "      <td>-0.107565</td>\n",
       "      <td>-0.055666</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.110602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.024499</td>\n",
       "      <td>0.040940</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.075582</td>\n",
       "      <td>0.068769</td>\n",
       "      <td>0.016823</td>\n",
       "      <td>-0.034788</td>\n",
       "      <td>-0.034706</td>\n",
       "      <td>-0.147462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.034114</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.086862</td>\n",
       "      <td>-0.079623</td>\n",
       "      <td>-0.014161</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>-0.063482</td>\n",
       "      <td>-0.028675</td>\n",
       "      <td>-0.180242</td>\n",
       "      <td>-0.034706</td>\n",
       "      <td>-0.179720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.130268</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-0.081157</td>\n",
       "      <td>-0.070541</td>\n",
       "      <td>-0.053338</td>\n",
       "      <td>-0.065802</td>\n",
       "      <td>-0.102899</td>\n",
       "      <td>0.137939</td>\n",
       "      <td>-0.127729</td>\n",
       "      <td>0.368667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.082191</td>\n",
       "      <td>-0.066903</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>-0.085758</td>\n",
       "      <td>-0.076476</td>\n",
       "      <td>-0.046370</td>\n",
       "      <td>-0.093644</td>\n",
       "      <td>-0.089018</td>\n",
       "      <td>0.065212</td>\n",
       "      <td>-0.197496</td>\n",
       "      <td>0.207376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0  fixed acidity  volatile acidity  citric acid  residual sugar   \n",
       "0     1.0       0.013963         -0.008080     0.015547        0.219457  \\\n",
       "1     1.0      -0.053345          0.021332     0.003499       -0.073488   \n",
       "2     1.0       0.119732          0.001724     0.039644        0.007800   \n",
       "3     1.0       0.033193         -0.047295    -0.008549        0.032340   \n",
       "4     1.0       0.033193         -0.047295    -0.008549        0.032340   \n",
       "...   ...            ...               ...          ...             ...   \n",
       "4893  1.0      -0.062960         -0.066903    -0.026621       -0.073488   \n",
       "4894  1.0      -0.024499          0.040940     0.015547        0.024672   \n",
       "4895  1.0      -0.034114         -0.037491    -0.086862       -0.079623   \n",
       "4896  1.0      -0.130268          0.011528    -0.020597       -0.081157   \n",
       "4897  1.0      -0.082191         -0.066903     0.027595       -0.085758   \n",
       "\n",
       "      chlorides  free sulfur dioxide  total sulfur dioxide   density   \n",
       "0     -0.002292             0.033770              0.073409  0.134425  \\\n",
       "1      0.009578            -0.074244             -0.014758 -0.000528   \n",
       "2      0.012545            -0.018495             -0.095964  0.020679   \n",
       "3      0.036284             0.040738              0.110532  0.030319   \n",
       "4      0.036284             0.040738              0.110532  0.030319   \n",
       "...         ...                  ...                   ...       ...   \n",
       "4893  -0.020096            -0.039401             -0.107565 -0.055666   \n",
       "4894   0.003643             0.075582              0.068769  0.016823   \n",
       "4895  -0.014161            -0.018495             -0.063482 -0.028675   \n",
       "4896  -0.070541            -0.053338             -0.065802 -0.102899   \n",
       "4897  -0.076476            -0.046370             -0.093644 -0.089018   \n",
       "\n",
       "            pH  sulphates   alcohol  \n",
       "0    -0.171151  -0.046334 -0.276495  \n",
       "1     0.101576   0.000178 -0.163591  \n",
       "2     0.065212  -0.057961 -0.066817  \n",
       "3     0.001576  -0.104473 -0.099075  \n",
       "4     0.001576  -0.104473 -0.099075  \n",
       "...        ...        ...       ...  \n",
       "4893  0.074303   0.011806  0.110602  \n",
       "4894 -0.034788  -0.034706 -0.147462  \n",
       "4895 -0.180242  -0.034706 -0.179720  \n",
       "4896  0.137939  -0.127729  0.368667  \n",
       "4897  0.065212  -0.197496  0.207376  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里注意一个小trick：回归系数会比特征x多一维，为了向量相乘方便，可以在训练集X左侧添加全为1的一列\n",
    "data0 = pd.concat([pd.DataFrame(np.ones(X.shape[0]), columns=['x0']), X], axis=1)\n",
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1852879 ],\n",
       "       [ 0.38811499],\n",
       "       [ 0.57586505],\n",
       "       [ 0.99815774],\n",
       "       [ 0.05425906],\n",
       "       [ 1.50600438],\n",
       "       [-0.04236977],\n",
       "       [ 1.27560261],\n",
       "       [-2.07555502],\n",
       "       [ 0.66357688],\n",
       "       [-0.34178232],\n",
       "       [ 0.38693489]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化回归系数\n",
    "W_init = np.random.randn(data0.shape[1], 1)\n",
    "W_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO：批量梯度下降\n",
    "## TODO：随机梯度下降\n",
    "## TODO：回归模型在机器学习和统计学上的差异\n",
    "## TODO：岭回归"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
